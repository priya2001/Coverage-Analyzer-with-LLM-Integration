"""LLM Integration Module for Test Suggestion Generation

This module integrates with Large Language Models (OpenAI, Anthropic) to generate
intelligent test suggestions based on uncovered coverage bins.
"""

import os
import json
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
from dotenv import load_dotenv

# Try importing LLM libraries
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

load_dotenv()


class DifficultyLevel(Enum):
    """Test difficulty levels"""
    EASY = "easy"
    MEDIUM = "medium"
    HARD = "hard"
    VERY_HARD = "very_hard"


@dataclass
class TestSuggestion:
    """Represents a test suggestion generated by LLM"""
    uncovered_bin: Dict[str, Any]
    description: str
    test_outline: List[str]
    difficulty: DifficultyLevel
    dependencies: List[str]
    reasoning: str
    estimated_time_hours: float
    priority_score: float = 0.0


class LLMTestGenerator:
    """Generates test suggestions using LLM"""
    
    def __init__(self, provider: str = "openai", model: str = "gpt-4"):
        """
        Initialize LLM test generator
        
        Args:
            provider: LLM provider ("openai" or "anthropic")
            model: Model name to use
        """
        self.provider = provider.lower()
        self.model = model
        
        if self.provider == "openai":
            if not OPENAI_AVAILABLE:
                raise ImportError("OpenAI library not installed. Install with: pip install openai")
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment variables")
            self.client = OpenAI(api_key=api_key)
        elif self.provider == "anthropic":
            if not ANTHROPIC_AVAILABLE:
                raise ImportError("Anthropic library not installed. Install with: pip install anthropic")
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                raise ValueError("ANTHROPIC_API_KEY not found in environment variables")
            self.client = anthropic.Anthropic(api_key=api_key)
        else:
            raise ValueError(f"Unsupported provider: {provider}")
    
    def generate_suggestions(
        self,
        uncovered_bins: List[Dict[str, Any]],
        design_name: str,
        design_context: Optional[str] = None,
        max_suggestions: Optional[int] = None
    ) -> List[TestSuggestion]:
        """
        Generate test suggestions for uncovered bins
        
        Args:
            uncovered_bins: List of uncovered bin dictionaries
            design_name: Name of the design/IP under test
            design_context: Optional context about the design
            max_suggestions: Maximum number of suggestions to generate
        
        Returns:
            List of TestSuggestion objects
        """
        suggestions = []
        bins_to_process = uncovered_bins[:max_suggestions] if max_suggestions else uncovered_bins
        
        for bin_info in bins_to_process:
            suggestion = self._generate_single_suggestion(bin_info, design_name, design_context)
            if suggestion:
                suggestions.append(suggestion)
        
        return suggestions
    
    def _generate_single_suggestion(
        self,
        bin_info: Dict[str, Any],
        design_name: str,
        design_context: Optional[str] = None
    ) -> Optional[TestSuggestion]:
        """Generate a single test suggestion for an uncovered bin"""
        
        prompt = self._build_prompt(bin_info, design_name, design_context)
        
        try:
            if self.provider == "openai":
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self._get_system_prompt()},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1500
                )
                content = response.choices[0].message.content
            else:  # anthropic
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=1500,
                    temperature=0.7,
                    system=self._get_system_prompt(),
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                content = response.content[0].text
            
            return self._parse_llm_response(content, bin_info)
        
        except Exception as e:
            print(f"Error generating suggestion for bin {bin_info.get('bin', 'unknown')}: {e}")
            return None
    
    def _get_system_prompt(self) -> str:
        """Get the system prompt for LLM"""
        return """You are an expert hardware verification engineer specializing in functional coverage analysis. 
Your task is to analyze uncovered coverage bins and generate actionable test suggestions to close coverage gaps.

When generating test suggestions, provide:
1. A clear description of the test scenario
2. A step-by-step test outline
3. An estimated difficulty level (easy, medium, hard, very_hard)
4. Any dependencies or prerequisites
5. Clear reasoning explaining why this test will hit the uncovered bin
6. Estimated time in hours

Format your response as JSON with the following structure:
{
    "description": "Clear description of the test scenario",
    "test_outline": ["step1", "step2", "step3"],
    "difficulty": "easy|medium|hard|very_hard",
    "dependencies": ["dependency1", "dependency2"],
    "reasoning": "Explanation of why this test will hit the uncovered bin",
    "estimated_time_hours": 4.0
}"""
    
    def _build_prompt(
        self,
        bin_info: Dict[str, Any],
        design_name: str,
        design_context: Optional[str] = None
    ) -> str:
        """Build the prompt for LLM"""
        
        prompt = f"""Design/IP Under Test: {design_name}
"""
        
        if design_context:
            prompt += f"Design Context: {design_context}\n\n"
        
        prompt += f"""Coverage Gap Analysis:

Covergroup: {bin_info.get('covergroup', 'N/A')}
Coverpoint: {bin_info.get('coverpoint', 'N/A')}
Uncovered Bin: {bin_info.get('bin', 'N/A')}
Current Hit Count: {bin_info.get('hit_count', 0)}
Coverpoint Coverage: {bin_info.get('coverage_percentage', 0):.2f}%

"""
        
        # Add context about related covered bins if available
        if 'related_bins' in bin_info:
            prompt += f"Related Covered Bins: {', '.join(bin_info['related_bins'])}\n\n"
        
        prompt += """Based on this information, generate a test suggestion that will help close this coverage gap.
Consider the design functionality, typical use cases, and edge cases that might trigger this specific bin."""
        
        return prompt
    
    def _parse_llm_response(
        self,
        response_text: str,
        bin_info: Dict[str, Any]
    ) -> Optional[TestSuggestion]:
        """Parse LLM response into TestSuggestion object"""
        
        try:
            # Try to extract JSON from response
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                # Fallback: create a basic suggestion
                return self._create_fallback_suggestion(response_text, bin_info)
            
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)
            
            # Map difficulty string to enum
            difficulty_str = data.get('difficulty', 'medium').lower()
            difficulty_map = {
                'easy': DifficultyLevel.EASY,
                'medium': DifficultyLevel.MEDIUM,
                'hard': DifficultyLevel.HARD,
                'very_hard': DifficultyLevel.VERY_HARD
            }
            difficulty = difficulty_map.get(difficulty_str, DifficultyLevel.MEDIUM)
            
            return TestSuggestion(
                uncovered_bin=bin_info,
                description=data.get('description', 'No description provided'),
                test_outline=data.get('test_outline', []),
                difficulty=difficulty,
                dependencies=data.get('dependencies', []),
                reasoning=data.get('reasoning', 'No reasoning provided'),
                estimated_time_hours=float(data.get('estimated_time_hours', 4.0))
            )
        
        except Exception as e:
            print(f"Error parsing LLM response: {e}")
            return self._create_fallback_suggestion(response_text, bin_info)
    
    def _create_fallback_suggestion(
        self,
        response_text: str,
        bin_info: Dict[str, Any]
    ) -> TestSuggestion:
        """Create a fallback suggestion when JSON parsing fails"""
        return TestSuggestion(
            uncovered_bin=bin_info,
            description=response_text[:200] if response_text else "Test suggestion generated",
            test_outline=["1. Setup test environment", "2. Configure DUT", "3. Execute test", "4. Verify coverage"],
            difficulty=DifficultyLevel.MEDIUM,
            dependencies=[],
            reasoning="Generated from LLM analysis",
            estimated_time_hours=4.0
        )


class MockLLMGenerator:
    """Mock LLM generator for testing without API access"""
    
    def generate_suggestions(
        self,
        uncovered_bins: List[Dict[str, Any]],
        design_name: str,
        design_context: Optional[str] = None,
        max_suggestions: Optional[int] = None
    ) -> List[TestSuggestion]:
        """Generate mock test suggestions"""
        suggestions = []
        bins_to_process = uncovered_bins[:max_suggestions] if max_suggestions else uncovered_bins
        
        for bin_info in bins_to_process:
            suggestion = TestSuggestion(
                uncovered_bin=bin_info,
                description=f"Test to cover {bin_info.get('bin', 'unknown')} in {bin_info.get('coverpoint', 'unknown')}",
                test_outline=[
                    f"1. Initialize {design_name}",
                    f"2. Configure {bin_info.get('coverpoint', 'coverpoint')}",
                    f"3. Trigger condition for {bin_info.get('bin', 'bin')}",
                    "4. Verify coverage hit"
                ],
                difficulty=DifficultyLevel.MEDIUM,
                dependencies=[f"{bin_info.get('covergroup', 'covergroup')} setup"],
                reasoning=f"This test targets the uncovered bin {bin_info.get('bin', 'unknown')} by exercising the specific condition required to hit this coverage point.",
                estimated_time_hours=4.0
            )
            suggestions.append(suggestion)
        
        return suggestions
